{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing all the libraries\n",
    "\n",
    "from selenium import webdriver \n",
    "import pandas as pd \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUTUBE SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A helper function to extract all the links for a particular topic\n",
    "\n",
    "def link(driver,links,file):\n",
    "    \n",
    "    time.sleep(15)\n",
    "    user_data = driver.find_elements_by_xpath('//*[@id=\"video-title\"]')\n",
    "    \n",
    "    for i in user_data:\n",
    "        if str(i.get_attribute('href')) not in links and str(i.get_attribute('href'))!='None':\n",
    "               links.append(str(i.get_attribute('href')))\n",
    "        else:\n",
    "               continue\n",
    "    print(len(links))\n",
    "    \n",
    "    if (len(links))>=170:\n",
    "        driver.close() ##close the browser window\n",
    "        a = open(file, 'w')\n",
    "        a.write(\" \".join(links))\n",
    "        a.close()\n",
    "        return links\n",
    "        \n",
    "    else:\n",
    "        driver.execute_script(\"window.scrollBy(0,4300)\",\"\") ##scrolls the window of browser vertically\n",
    "        link(driver,links,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A function to extract all the links for a particular topic\n",
    "\n",
    "def get_links(data,links,file):\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path = 'G:\\youtube scraping\\chromedriver_win32\\chromedriver.exe')\n",
    "    ## It open a chrome browser\n",
    "    \n",
    "    driver.get(\"https://www.youtube.com/results?search_query=\" + str(data))\n",
    "    ## browser is opened with this link\n",
    "    \n",
    "    link(driver,links,file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A function to get all the details of a particular topic\n",
    "\n",
    "def get_df(link,data):\n",
    "    \n",
    "        driver = webdriver.Chrome(executable_path = 'G:\\youtube scraping\\chromedriver_win32\\chromedriver.exe')\n",
    "        driver.get(link)\n",
    "        ## It open a chrome browser with a particular link\n",
    "            \n",
    "        wait = WebDriverWait(driver,45)\n",
    "        \n",
    "        v_id = link.strip('https://www.youtube.com/watch?v=') ## extracts video_id \n",
    "           \n",
    "        v_title = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"h1.title yt-formatted-string\"))).text\n",
    "        ## extracts video_title\n",
    "                \n",
    "        v_description =  wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div#description yt-formatted-string\"))).text\n",
    "        ## extract video_description\n",
    "        \n",
    "        v_category=data\n",
    "            \n",
    "        driver.close() ## closes the browser window\n",
    "        row=  [v_id, v_title, v_description, v_category]  \n",
    "        \n",
    "        return row\n",
    "    \n",
    "            \n",
    "    \n",
    "     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting links for every topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "43\n",
      "56\n",
      "69\n",
      "83\n",
      "94\n",
      "106\n",
      "116\n",
      "129\n",
      "143\n",
      "155\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "data_t='Travel Blogs'\n",
    "links_tb = []\n",
    "links_Travel_Blogs=get_links(data_t,links_tb,'links_Travel_Blogs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "34\n",
      "51\n",
      "65\n",
      "77\n",
      "92\n",
      "103\n",
      "121\n",
      "137\n",
      "150\n",
      "165\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "data_st='Science and Technology'\n",
    "links_st = []\n",
    "links_Science_and_Technology=get_links(data_st,links_st,'links_Science_and_Technology.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "38\n",
      "57\n",
      "75\n",
      "92\n",
      "109\n",
      "128\n",
      "145\n",
      "161\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "data_f='Food'\n",
    "links_f = []\n",
    "links_Food=get_links(data_f,links_f,'links_Food.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "36\n",
      "56\n",
      "72\n",
      "91\n",
      "108\n",
      "125\n",
      "140\n",
      "154\n",
      "169\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "data_m='Manufacturing'\n",
    "links_m = []\n",
    "links_Manufacturing=get_links(data_m,links_m,'links_Manufacturing.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "41\n",
      "61\n",
      "80\n",
      "98\n",
      "118\n",
      "136\n",
      "155\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "data_h='History'\n",
    "links_h = []\n",
    "links_History=get_links(data_h,links_h,'links_History.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "35\n",
      "48\n",
      "62\n",
      "78\n",
      "93\n",
      "105\n",
      "121\n",
      "139\n",
      "156\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "data_am='Art_and_Music'\n",
    "links_am = []\n",
    "links_Art_and_Music=get_links(data_am,links_am,'links_Art_and_Music.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Details of all the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel_Blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_Travel_Blogs=[]\n",
    "for i in range(170):\n",
    "    list_Travel_Blogs.append(get_df(links_Travel_Blogs[i],'Travel_Blogs'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Science_and_Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_Science_and_Technology=[]\n",
    "for i in range(70):\n",
    "    list_Science_and_Technology.append(get_df(links_Science_and_Technology[i],'Science_and_Technology'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manufacturing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_Manufacturing=[]\n",
    "for i in range(170):\n",
    "    list_Manufacturing.append(get_df(links_Manufacturing[i],'Manufacturing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_Food=[]\n",
    "for i in range(171):\n",
    "    list_Food.append(get_df(links_Food[i],'Food'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_History=[]\n",
    "for i in range(170):\n",
    "    list_History.append(get_df(links_History[i],'History'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Art_and_Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_Art_and_Music=[]\n",
    "for i in range(170):\n",
    "    list_Art_and_Music.append(get_df(links_Art_and_Music[i],'Art_and_Music'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding all the data in a list \n",
    "l=list_Travel_Blogs+list_Art_and_Music+list_History+list_Science_and_Technology+list_Food+list_Manufacturing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import all the libraries\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_words=stopwords.words('english')+list(string.punctuation)\n",
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  finding pos\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):##adjective\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):##verb\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):##noun\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):##adverb\n",
    "        return 'r'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cleaning the document\n",
    "\n",
    "def clean_doc(doc):\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(doc))\n",
    "    #print(tagged)\n",
    "    new_data=[]\n",
    "    for i,j in tagged:\n",
    "        if i.lower() not in extra_words and i.isalpha()==True and len(i)>2:\n",
    "            try:\n",
    "                a=get_wordnet_pos(j)\n",
    "                new_word=lemm.lemmatize(i.lower(),pos=a)\n",
    "                new_data.append(new_word.lower())\n",
    "            except:\n",
    "                new_data.append(i.lower())\n",
    "            \n",
    "            \n",
    "    return ' '.join(new_data),new_data\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning the title\n",
    "for i in l:\n",
    "    i[1]=clean_doc(i[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning the description\n",
    "all_words=[]\n",
    "doc=[]\n",
    "for i in l:\n",
    "    i[2]=clean_doc(i[2])[0]\n",
    "    all_words+=clean_doc(i[2])[1]\n",
    "    doc.append((clean_doc(i[2])[0],i[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http stay current recent uploads update channel youtube', 'Art_and_Music')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(l,columns = ['Video id',' Title',' Description',' Category' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JLZlCZ0</td>\n",
       "      <td>travelling north east india arunachal journey ...</td>\n",
       "      <td>journey arunachal north east india begin train...</td>\n",
       "      <td>Travel_Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i9E_Blai8vk</td>\n",
       "      <td>welcome bali travel vlog priscilla lee</td>\n",
       "      <td>chance fly bali whole family thanksgiving firs...</td>\n",
       "      <td>Travel_Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n79Rv0F48</td>\n",
       "      <td>backpack meghalaya northeast india trip sohra ...</td>\n",
       "      <td>video explore north east india start guwahati ...</td>\n",
       "      <td>Travel_Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i5F7Xh9CO8U</td>\n",
       "      <td>exploring varanasi benaras travel vlog</td>\n",
       "      <td>spent incredible week explore beautiful city v...</td>\n",
       "      <td>Travel_Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RcmrbNRK-jY</td>\n",
       "      <td>day trip around world travel film</td>\n",
       "      <td>wife travel country day film story incredible ...</td>\n",
       "      <td>Travel_Blogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Video id                                              Title  \\\n",
       "0      JLZlCZ0  travelling north east india arunachal journey ...   \n",
       "1  i9E_Blai8vk             welcome bali travel vlog priscilla lee   \n",
       "2    n79Rv0F48  backpack meghalaya northeast india trip sohra ...   \n",
       "3  i5F7Xh9CO8U             exploring varanasi benaras travel vlog   \n",
       "4  RcmrbNRK-jY                  day trip around world travel film   \n",
       "\n",
       "                                         Description      Category  \n",
       "0  journey arunachal north east india begin train...  Travel_Blogs  \n",
       "1  chance fly bali whole family thanksgiving firs...  Travel_Blogs  \n",
       "2  video explore north east india start guwahati ...  Travel_Blogs  \n",
       "3  spent incredible week explore beautiful city v...  Travel_Blogs  \n",
       "4  wife travel country day film story incredible ...  Travel_Blogs  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('youtubeScrap.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ys=pd.read_csv('youtubeScrap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Video id', ' Title', ' Description', ' Category'], dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ys.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'journey arunachal north east india begin train journey guwahati murkongselek head pasighat travel companion get start explore tiny glimpe arunachal far market bridge adventure get well next video subscribe channel hit bell icon stay notified follow social medium instagram facebook twitter website check gear kit music epidemic music hear epidemic sound sign use link get month free subscription trip unknown arunachal pradesh november november organise chalohoppo travel company specialise craft authentic local experience northeast india experience raw nothing like anything experience check instagram facebook website shot traveller edit premiere pro'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ys.loc[0,' Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=data_ys[' Description']\n",
    "target=data_ys[' Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import countvectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split data into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainX,testX,trainY,testY=train_test_split(text,target,random_state=1,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vex=CountVectorizer(max_features=3000, analyzer='word', encoding = \"ISO-8859-1\")\n",
    "train_feature=count_vex.fit_transform(trainX.values.astype('U'),trainY)\n",
    "test_feature=count_vex.transform(testX.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varsha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Varsha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "lr.fit(train_feature,trainY)\n",
    "y_score=lr.score(test_feature,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8275862068965517"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
